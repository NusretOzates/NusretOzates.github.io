[
  {
    "objectID": "posts/test_time_compute/index.html",
    "href": "posts/test_time_compute/index.html",
    "title": "Test-Time Compute, Reasoning and Human Brain",
    "section": "",
    "text": "I have lots of things to do, but I‚Äôve suddenly been struck by inspiration, and since the well-known work-avoidance mechanism has kicked in, I‚Äôm going to write down my thoughts on test-time compute, shared decoders, and reasoning. Here we go, a theoretical and lengthy piece is coming.\nOne of the things I love about LLMs is that they handle multiple tasks with a single loss and a single branch. That is, instead of a shared encoder + separate decoders for each task like in U-net models, there‚Äôs just a transformer decoder. As far as I know, this comes from T5 models where we model all NLP tasks as text-to-text. We‚Äôll get back to LLMs in a bit, but let‚Äôs take a look at the vision side for now.\nI don‚Äôt think there‚Äôs an equivalent of this in vision, for example, how would you combine segmentation and classification tasks? At the very least, the last layers would have to be different. However, there‚Äôs a model that comes very, very close to combining these, and the model‚Äôs developers are inspired by the human brain. The model is called BU-TD. Let‚Äôs start with the inspiration part. These folks are saying, ‚Äòsegmenting anything and everything at once‚Äô is not the right approach; the human brain doesn‚Äôt work that way.\nFor example, the longer you look at the image above, the more details emerge; the brain doesn‚Äôt grasp the entire image with all its details at once. So why are we trying to make models do this? This is where BU-TD comes into play.\nAgain, in this model inspired by the human brain, first an encoder processes the image, and the decoder takes the vector coming from the encoder and additionally receives a task and argument vector. For example,\nWe tell the model to look at the image, task: find hairstyle, argument: bob. Afterwards, the decoder can optionally output the segmentation of Bob‚Äôs hair, but it may not; that part is a bit vague. By feeding the decoder‚Äôs result back into the encoder, we get the result corresponding to that style, and so on.\nThe nice thing about this system is that it first uses the weights in the decoder to the fullest, making it a parameter-efficient model. Secondly, since each task, argument, etc., is decoupled, the system learns concepts better. For example, during training, brother Bob is always bald, so Bob‚Äôs hairstyle value is always bald during training. However, since the model learns what short hair and long hair are from other examples independently of the person, it can detect this during testing if Bob comes to Turkey and gets hair implants. Thanks to this, the model learns much better with much less data.\nNow we come back to LLMs. We are actually applying the TD part of this model, that is, using a single decoder for each task, in LLMs since the T5 models, especially in PrefixLM models, and when image tokens and (I think) the question are processed with self-attention, the TD logic is formed.\nThe second part is that when people look at a picture, we extract the details, relationships, etc. over time; all the details don‚Äôt come at a single glance, right? Well, this part actually corresponds to the concept we call test-time compute. For example, as we look at the image on the left below, the details on the right emerge.\nBased on what we‚Äôve learned up to this point, trying to process everything at once is not logical. Learning the relationships one by one during training and processing the image over time using more compute/time during testing is an effective solution in terms of both the number of parameters and learning more with less data. Our next problem is this: We don‚Äôt want to process the image all at once, okay, but we also don‚Äôt want to process the entire image; we want to process as much as necessary for the information we are interested in to save time and money. This is where reasoning comes into play. What does a good LLM model do in terms of reasoning? It divides the question we ask into the necessary parts and solves the parts step by step, and stops when it reaches the result. By doing this, we have the following system:\nI‚Äôm saying that if I were shown a picture and asked, ‚ÄòWhat is the size of the bag of the woman holding the bag that the girl is looking at?‚Äô, my brain would process the image in a similar way and wouldn‚Äôt look for more details.\nI think the BU-TD model itself is very limited in terms of input and output ranges but the main idea is still strong and I believe VLMs are very close to implementing this idea into the AI models.\nThat‚Äôs it, I couldn‚Äôt come to a conclusion with the text. These things suddenly came to my mind while reading a very unrelated article, I thought I‚Äôd write them down. Good luck and thanks for reading up to this point!"
  },
  {
    "objectID": "posts/test_time_compute/index.html#references",
    "href": "posts/test_time_compute/index.html#references",
    "title": "Test-Time Compute, Reasoning and Human Brain",
    "section": "References",
    "text": "References\n\nBU-TD Model: ‚ÄúImage interpretation by iterative bottom-up top-down processing‚Äù\nT5 Model: ‚ÄúExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer‚Äù"
  },
  {
    "objectID": "posts/gnn_basics/index.html",
    "href": "posts/gnn_basics/index.html",
    "title": "Pytorch Geometric Basics: How Message Passing Works",
    "section": "",
    "text": "I‚Äôm working with GNNs for my MSc thesis and naturally chose PyTorch Geometric (PyG), one of the most popular libraries in the field. While PyG is incredibly easy to use, I realized I needed to understand how message passing works under the hood to effectively customize it for my specific experiments. Now that I‚Äôve gained this understanding, I‚Äôve written this post to share the inner workings of PyG with anyone else looking to build their own custom layers."
  },
  {
    "objectID": "posts/gnn_basics/index.html#conclusion",
    "href": "posts/gnn_basics/index.html#conclusion",
    "title": "Pytorch Geometric Basics: How Message Passing Works",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we explored how message passing works in PyTorch Geometric by breaking down the key methods of the MessagePassing class: propagate, message, aggregate, and update. If you want to customize your GNN architecture and experiment with different message-passing schemes, understanding these methods is critical. With this knowledge, you can implement your own GNN layers and tailor them to your specific needs. I will drop a simple working code that I‚Äôve borrowed from the PyG documentation below for reference.\nfrom typing import Optional\n\nimport torch\nfrom torch import Tensor\nfrom torch.nn import Linear, Parameter\nfrom torch_geometric.nn import MessagePassing\nfrom torch_geometric.utils import add_self_loops, degree\nfrom torch_geometric.data import Data\n\nclass GCNConv(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n        self.lin = Linear(in_channels, out_channels, bias=False)\n        self.bias = Parameter(torch.empty(out_channels))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.lin.reset_parameters()\n        self.bias.data.zero_()\n\n    def forward(self, x, edge_index):\n        # x has shape [N, in_channels]\n        # edge_index has shape [2, E]\n        print(\"Forward pass...\")\n        print(f\"x shape: {x.shape}\")\n        print(f\"edge_index shape: {edge_index.shape}\")\n\n        # Step 1: Add self-loops to the adjacency matrix.\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n\n        # Step 2: Linearly transform node feature matrix.\n        x = self.lin(x)\n\n        # Step 3: Compute normalization.\n        source, target = edge_index\n        deg = degree(target, x.size(0), dtype=x.dtype)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        norm = deg_inv_sqrt[source] * deg_inv_sqrt[target]\n\n        # Step 4-5: Start propagating messages.\n        out = self.propagate(edge_index, x=x, norm=norm)\n\n        # Step 6: Apply a final bias vector.\n        out = out + self.bias\n\n        return out\n\n    def message(self, x_i, x_j, norm, edge_index):\n        # x_j has shape [E, out_channels]\n        print(\"Creating messages...\")\n        print(f\"x_i shape: {x_i.shape}\")\n        print(f\"x_j shape: {x_j.shape}\")\n        print(f\"norm shape: {norm.shape}\")\n\n        # Step 4: Normalize node features.\n        return norm.view(-1, 1) * x_j\n\n    def aggregate(\n        self,\n        inputs: Tensor,\n        index: Tensor,\n        ptr: Optional[Tensor] = None,\n        dim_size: Optional[int] = None,\n    ) -&gt; Tensor:\n        print(\"Aggregating messages...\")\n        print(f\"Inputs shape: {inputs.shape}\")\n        print(f\"Index shape: {index.shape}\")\n        print(index)\n        return super().aggregate(inputs, index, ptr, dim_size)\n\n    def update(self, inputs: Tensor) -&gt; Tensor:\n        print(\"Updating node embeddings...\")\n        print(f\"Inputs shape: {inputs.shape}\")\n        print(inputs)\n        return super().update(inputs)\n\n\nedge_index = torch.tensor([[0, 1],\n                           [1, 0],\n                           [1, 2],\n                           [2, 1]], dtype=torch.long)\nx = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n\ndata = Data(x=x, edge_index=edge_index.t().contiguous())\n\nconv = GCNConv(1, 2)\nout = conv(data.x, data.edge_index)\nprint(out)"
  },
  {
    "objectID": "posts/gnn_basics/index.html#references",
    "href": "posts/gnn_basics/index.html#references",
    "title": "Pytorch Geometric Basics: How Message Passing Works",
    "section": "References",
    "text": "References\n\nPyTorch Geometric Documentation\nFor GNN basics, and also for the headline image reference\nAlso, a great YouTube video for GNN basics from Stanford\n\nThanks for coming so far, have fun!"
  },
  {
    "objectID": "posts/career_advice_ai_summary/index.html",
    "href": "posts/career_advice_ai_summary/index.html",
    "title": "My summary of the ‚ÄúCareer Advice in AI‚Äù Lecture",
    "section": "",
    "text": "I finally had time to watch the AI Career Advice lesson by Andrew Ng and Laurence Moroney. It was a great lesson for me as I‚Äôm about to graduate from my MSc and am ready to join the industry again! I‚Äôve created a summary and wanted to share it with everyone, but I strongly recommend watching it.\nIn Andrew Ng‚Äôs introduction, he mentions two important points. The first one is that, although saying this is (according to some people) politically incorrect, working hard is important for success. But there definitely are some exceptions, like when you have an injury, you just have a kid, and examples like that. The second one is about surrounding yourself with bright minds, high-quality people, both in your personal life and your work life, as you are the average of your surroundings. Choosing with whom you work is more important than where you work. Additionally, he mentioned that AI makes engineers faster, but the ones who also listen the user feedback, communicate with other people will be the fastest ones.\nLaurence Moroney makes crucial and thoughtful additions to these. About the hard working part, he said hard work ‚â† amount of time spent. It must be something measurable, like what is your output after those hours? X new products? Y papers read and understand the papers properly? And about your surroundings, he reminded you that those people also will choose if they want to see you around. Even if you are a 10x engineer, if you are a rude person, people won‚Äôt want to see you. After these additions, he talked about the 3 pillars of success that you need to show to the employers, not just tell."
  },
  {
    "objectID": "posts/career_advice_ai_summary/index.html#understanding-depth",
    "href": "posts/career_advice_ai_summary/index.html#understanding-depth",
    "title": "My summary of the ‚ÄúCareer Advice in AI‚Äù Lecture",
    "section": "Understanding Depth",
    "text": "Understanding Depth\nSurface-level knowledge is not enough anymore. You need to have academic knowledge, diverse skills, and the ability to separate noise from the real trends because engagement is the currency of social media, not the accuracy, so there will be a lot of noise there. By diverse skills, he doesn‚Äôt mean knowing both about NLP and CV; he means knowing about training ML models while also knowing about how to deploy them, scale them, and build an application on them to be valuable even if the AI hype completely deflates tomorrow. He also gave a practical strategy to filtering noise: Develop trusted sources and filter them actively. Learn more about the fundamentals of the hyped tech (enters the academic knowledge) before judging its impact (e.g., Hollywood is over, SWE is over). And always aware of the trends and know why it is a trend right now. As an example, think about ‚ÄúAI Agents‚Äù before directly going into implementation, understand how they work, ‚Äúwhen‚Äù and ‚Äúwhy‚Äù it adds value, and when it won‚Äôt add any value. And how will it help?"
  },
  {
    "objectID": "posts/career_advice_ai_summary/index.html#business-focus",
    "href": "posts/career_advice_ai_summary/index.html#business-focus",
    "title": "My summary of the ‚ÄúCareer Advice in AI‚Äù Lecture",
    "section": "Business Focus",
    "text": "Business Focus\nYou need to translate the capabilities of AI into a real business outcome. If you go directly with the hype, aka agents, these days, you will fail. First, you need to peel apart the business requirements, ask ‚Äúwhy‚Äù and ‚Äúwhat‚Äù a lot of times to understand the real bottleneck/problem. Additionally, risks of mispredictions, hallucinations, biases, and misuses (some edge cases you will never think of will be found by the users) are here and will stay here. Knowing how to manage those risks while making a process an AI-enabled process is critical."
  },
  {
    "objectID": "posts/career_advice_ai_summary/index.html#bias-towards-delivery",
    "href": "posts/career_advice_ai_summary/index.html#bias-towards-delivery",
    "title": "My summary of the ‚ÄúCareer Advice in AI‚Äù Lecture",
    "section": "Bias Towards Delivery",
    "text": "Bias Towards Delivery\nBuilding cool things that have no value is not that important anymore (as it was when hype started). You need to build useful things; if it is both useful and cool, it is definitely better. Show that you can ship working solutions more than demos. An example from him: Before applying to Google Cloud, while he was writing a Java book, he made a Java application that runs on the Google Cloud and showed it on the interview, which turned the interview process into questions about his app instead of weird questions like how many windows in New-York.\nAnd some additional points:\n\nYou will make mistakes, so learn from them and also be helpful when someone else makes a mistake\nVibe coding is good unless you mindlessly copy-paste the code. Every time you use AI to generate code, you are taking technical debt.\nLearning how to fine-tune those small LLMs is currently one of the most important things, due to privacy reasons in a lot of industries\n\nIf you want to watch:"
  },
  {
    "objectID": "notes/slurm/index.html",
    "href": "notes/slurm/index.html",
    "title": "Slurm Notes",
    "section": "",
    "text": "Here I will share the useful things I learned when using the SLURM cluster of my university.\nThis bash command shows the free memory, free number of CPUs, and used GPUs on the nodes in the ‚Äúai‚Äù partition. You can use it to see available resources before submitting a job, which makes it easier to start jobs faster.\nsinfo -O \"NodeHost,FreeMem,CPUsState,GresUsed\" -p ai"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi Everyone!",
    "section": "",
    "text": "As a Machine Learning Engineer, I build and refine end-to-end ML systems, handling tasks from data collection to model deployment on cloud platforms, with a focus on practical, scalable solutions. I have 4 years of experience in Natural Language Processing, and even surprisingly to me, I am working on Graph Neural Networks, Vision Transformers, and WSI Images for 3 years! I can even detect cancerous cells in pancreatic histology images by my eyes now‚Ä¶\nI am also skilled in MLOps, using tools such as Kubeflow, FastAPI, and Transformers to automate and optimise the ML lifecycle. Previously, I worked as an MLOps Engineer and a Senior Machine Learning Engineer at GLOSA and Carbon Consulting, respectively, where I implemented an AutoNLP platform and led the development team of the AI department.\nI hold a Master of Science in Computer Science from Koc University and several certifications, such as Google Cloud Certified Professional Machine Learning Engineer and Certified Spark NLP Data Scientist. I am passionate about applying AI to enhance user experience and support data-driven decision-making across company services."
  },
  {
    "objectID": "index.html#my-published-articles",
    "href": "index.html#my-published-articles",
    "title": "Hi Everyone!",
    "section": "My Published Articles",
    "text": "My Published Articles\n‚Ä¶ TBD ‚Ä¶"
  },
  {
    "objectID": "index.html#what-is-what",
    "href": "index.html#what-is-what",
    "title": "Hi Everyone!",
    "section": "What is What?",
    "text": "What is What?\nIn the blogs section, I share whatever I learn about in a kind of detailed, structered and friendly way. I am also trying to move my blog posts from Medium to here, so you can find my previous articles as well.\nIn the notes section, I decided share quick notes and commands that I find useful for my daily work."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card‚Äôs content.\n\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card‚Äôs content.\n\n\n\n\n\n\n\nSome quick example text to build on the card title and make up the bulk of the card‚Äôs content.\n\n\n\n\n\n  \n  \n    Some quick example text to build on the card title and make up the bulk of the card's content."
  },
  {
    "objectID": "myposts.html",
    "href": "myposts.html",
    "title": "Welcome to My Posts",
    "section": "",
    "text": "My summary of the ‚ÄúCareer Advice in AI‚Äù Lecture\n\n\n\nml\n\n\n\n\n\n\n\n\n\nDec 20, 2025\n\n\nNusret Ozates\n\n\n\n\n\n\n\n\n\n\n\n\nPytorch Geometric Basics: How Message Passing Works\n\n\n\nml\n\n\n\n\n\n\n\n\n\nOct 18, 2025\n\n\nNusret Ozates\n\n\n\n\n\n\n\n\n\n\n\n\nAcademic Writing Notes: Paragraphs Development and Sentence Skills\n\n\n\nacademy\n\n\n\n\n\n\n\n\n\nOct 13, 2025\n\n\nNusret Ozates\n\n\n\n\n\n\n\n\n\n\n\n\nTest-Time Compute, Reasoning and Human Brain\n\n\n\nml\n\nllm\n\n\n\n\n\n\n\n\n\nFeb 7, 2025\n\n\nNusret Ozates\n\n\n\n\n\n\n\n\n\n\n\n\nThings you need to know about Docker to get started\n\n\n\ndevops\n\nmlops\n\nsoftware\n\n\n\n\n\n\n\n\n\nNov 8, 2020\n\n\nNusret Ozates\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Quick Notes",
    "section": "",
    "text": "Slurm Notes\n\n\n\ngeneral\n\n\n\n\n\n\n\n\n\nNov 12, 2025\n\n\nNusret Ozates\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/docker-get-started/index.html",
    "href": "posts/docker-get-started/index.html",
    "title": "Things you need to know about Docker to get started",
    "section": "",
    "text": "Some useful commands and concepts to use Docker!\nThe original video that I take notes from :\nIn this article, I‚Äòll talk about Docker. We will begin from why we need to use it, to how do we manage multiple Docker containers at the same time."
  },
  {
    "objectID": "posts/docker-get-started/index.html#why-do-we-need-docker",
    "href": "posts/docker-get-started/index.html#why-do-we-need-docker",
    "title": "Things you need to know about Docker to get started",
    "section": "Why do we need Docker?",
    "text": "Why do we need Docker?\nWe have web servers, database services, messaging services, etc. and all of them have their dependencies(libraries, OS version, etc.) and there can be a conflict between them. We call it ‚ÄúThe matrix from Hell‚Äù.\n\n\n\nThe matrix from Hell"
  },
  {
    "objectID": "posts/docker-get-started/index.html#what-does-docker-do",
    "href": "posts/docker-get-started/index.html#what-does-docker-do",
    "title": "Things you need to know about Docker to get started",
    "section": "What does docker do?",
    "text": "What does docker do?\nRun each component in a separate, isolated environment with its dependencies and its libraries. All within the same VM or host."
  },
  {
    "objectID": "posts/docker-get-started/index.html#what-are-the-differences-with-vm",
    "href": "posts/docker-get-started/index.html#what-are-the-differences-with-vm",
    "title": "Things you need to know about Docker to get started",
    "section": "What are the differences with VM?",
    "text": "What are the differences with VM?\nVMs are complete isolation! They have their hardware, kernel, and OS. But docker containers use the same hardware and same Linux kernel.\n\nThat is the reason why you can‚Äôt have a Windows container. You can say: ‚ÄúHey! I have a docker on windows!‚Äù. Then I say, look for WSL. üòÑ\nContainers meant to run a specific task or process, not meant to host an OS.\n\n\n\n\nVirtual Machines vs Containers"
  },
  {
    "objectID": "posts/docker-get-started/index.html#some-useful-docker-commands",
    "href": "posts/docker-get-started/index.html#some-useful-docker-commands",
    "title": "Things you need to know about Docker to get started",
    "section": "Some Useful Docker Commands",
    "text": "Some Useful Docker Commands\n\ndocker version: It gives the docker version.\ndocker run: It is used to run a container from an image\n\ndocker run nginx ‚áí Runs instance of the Nginx application on the docker host\ndocker run -d nginx ‚áí Runs in the detached mode. That means the container will run in the background, and you can continue to use the terminal\ndocker run ‚Äî name webapp nginx ‚áí Run a container with the given name\ndocker run -it nginx ‚áí ‚Äú-i‚Äù gives stdin to docker, you can get input from the terminal. ‚Äú-t‚Äù gives terminal so your dockerized app can print something\ndocker run -v /opt/datadir:/var/lib/mysql ‚Ä¶.. ‚áí The container maps /var/lib/mysql(in docker) to /opt/datadir(in your pc). Your data will persist even when you delete the container.\ndocker run -p 80:5000 nginx ‚áí Forward your port 80 to container‚Äôs port 5000.\n\n\nNote: You can‚Äôt bind the same host port to the multiple docker instances.\n\n\ndocker ps: List all running containers and several key information about them. If used with the ‚Äú-a‚Äù parameter, you can see previously stopped or exited containers.\ndocker stop: It stops the running containers. Needs container ID or name.\n\ndocker stop silly_sammet\n\ndocker rm: Removes stopped or exited container permanently. If it prints the name back, we are good.\n\ndocker rm silly_sammet\n\ndocker images: Gives a list of downloaded images and their sizes.\ndocker rmi: Removes the given image. You need to remove all dependent containers before.\n\ndocker rmi nginx\n\ndocker pull: Just downloads the images so you won‚Äôt wait when you want to run the image.\ndocker exec: Execute a command in the container.\n\ndocker exec distracted_meclintock(container name) cat /etc/host(command)\n\ndocker inspect: It returns all details of the container in JSON format.\n\ndocker inspect webapp\n\ndocker logs: This shows the logs of a container. It is useful when your container runs in detached mode"
  },
  {
    "objectID": "posts/docker-get-started/index.html#tags",
    "href": "posts/docker-get-started/index.html#tags",
    "title": "Things you need to know about Docker to get started",
    "section": "Tags",
    "text": "Tags\nFor example ‚Äúdocker redis‚Äù command will run the latest Redis version for you. What if you want to use an older version of Redis?\ndocker run redis:4.0 bold part is the Tag of a container.\nWhere can I find tags of the docker image?\nhttps://hub.docker.com\nEnvironment Variables\nIn python we access an environment variable like this:\nos.environ.get(‚ÄòAPP_COLOR‚Äô)\nHow can you set it in docker?\ndocker run -e APP_COLOR=pink web-app\nHow to create my own image?\nLet‚Äôs say we have a webserver to run on an Ubuntu OS, what would be our steps to run it?\n\nOS ‚Äî Ubuntu\nUpdate apt repo\nInstall dependencies using apt\nInstall Python dependencies using pip\nCopy source code to ex. /opt folder\nRun web server using ex. ‚Äúflask‚Äù command\n\nThen we need to do these steps in a file called Dockerfile.\nFROM Ubuntu  \nRUN apt-get update \nRUN apt-get install python  \nRUN pip install flask \nRUN pip install flask-mysql  \nCOPY . /opt/source-code  \nENTRYPOINT FLASK_APP = /opt/source-code/app.py flask run\nLet‚Äôs build our Dockerfile and have a docker image!\ndocker build -t nusret/chatbot \"Address of the dockerfile without double quote\"\nAnd push it to the DockerHub if you want\ndocker push nusret/chatbot"
  },
  {
    "objectID": "posts/docker-get-started/index.html#what-is-this-dockerfile",
    "href": "posts/docker-get-started/index.html#what-is-this-dockerfile",
    "title": "Things you need to know about Docker to get started",
    "section": "What is this Dockerfile?",
    "text": "What is this Dockerfile?\n\nDockerfile is a text file written in a specific format that docker can understand."
  },
  {
    "objectID": "posts/docker-get-started/index.html#how-can-i-exportimport-my-docker-image-as-a-tar-file",
    "href": "posts/docker-get-started/index.html#how-can-i-exportimport-my-docker-image-as-a-tar-file",
    "title": "Things you need to know about Docker to get started",
    "section": "How can I export/import my docker image as a tar file?",
    "text": "How can I export/import my docker image as a tar file?\nYou can export your Docker Image as a .tar file with this command:\ndocker save ‚Äîoutput chatbot.tar nusret/chatbot\nAnd you can easily import it with a very similar command.\ndocker load ‚Äîinput chatbot.tar"
  },
  {
    "objectID": "posts/docker-get-started/index.html#entrypoint-vs-cmd",
    "href": "posts/docker-get-started/index.html#entrypoint-vs-cmd",
    "title": "Things you need to know about Docker to get started",
    "section": "ENTRYPOINT VS CMD",
    "text": "ENTRYPOINT VS CMD\nLet‚Äôs say we have a docker container that just ‚Äúsleeps‚Äù named ‚Äúsleeper‚Äù. The docker file would be like this:\nFROM Ubuntu  \nCMD [\"sleep\",\"5\"]\nWhen I run the command:\ndocker run sleeper sleep 10\nThis CMD command will get replaced with sleep 10. But as this is a sleeper container, I could only say ‚Äú10‚Äù and the container must sleep. To do this we change the dockerfile like this:\nFROM Ubuntu  \nENTRYPOINT [\"sleep\"]\nThis time when I run:\ndocker run sleeper 10\nThe ‚Äú10‚Äù will be appended to the ‚Äúsleep‚Äù command and I can just set the sleep time. But what if I don‚Äôt write any number? How can I add a default sleep time?\nFROM Ubuntu  \nENTRYPOINT [\"sleep\"]\nCMD [\"5\"]"
  },
  {
    "objectID": "posts/paragraph_dev_sentece_skill/index.html",
    "href": "posts/paragraph_dev_sentece_skill/index.html",
    "title": "Academic Writing Notes: Paragraphs Development and Sentence Skills",
    "section": "",
    "text": "Do you remember the times you read an article/paper and you couldn‚Äôt understand what the author wanted to say even though you know the underlying concepts? Even the most brilliant ideas can be lost in poor writing or simplest ideas can be hard to understand. Choppy paragraphs, misplaced phrases, and grammatical run-ons can obscure your argument and frustrate your reader.\nIn this post, I will share what I learned from Koc University Academic Writing class videos and materials."
  },
  {
    "objectID": "posts/paragraph_dev_sentece_skill/index.html#topic-and-stress",
    "href": "posts/paragraph_dev_sentece_skill/index.html#topic-and-stress",
    "title": "Academic Writing Notes: Paragraphs Development and Sentence Skills",
    "section": "Topic and Stress",
    "text": "Topic and Stress\nYou can divide a sentence into two parts: the topic and the stress. The topic is what the sentence is about, and the stress is what you want to say about the topic or what is new information.\n\n\n\n\n\n\n\nTopic Part, Readers:\nStress Part, Readers\n\n\n\n\n- Expect to understand what the sentence is about.\n- Expect to see new and imporant ideas.\n\n\n- Try to connect the sentence to what they have already read.\n- Focus most of their interpretative effort.\n\n\n\nExample:\nAccounts of depression evolved after psychologists introduced the concepts of defeat and entrapment.\n\nKnown-New Contract\n\nIntroduce your readers to the ‚Äúbig picture‚Äù first by giving them information they already know (the topic part).\nThen they can link what‚Äôs familiar to the new information you give them (the stress part).\n\nAs that new information becomes familiar, it too becomes old information that can link to newer information.\nExample:\nAccounts of depression evolved after psychologists introduced the concepts of defeat and entrapment. These concepts have been implicated in theoretical accounts of anxiety and suicide. Such theories..\nExample 2:\nHowever, managed WebRTC services using SFU architecture and SDN-assisted IP multicasting of scalable video within WebRTC system are discussed for the first time in this paper\n\n\n\n\n\n\nImportant\n\n\n\nThe sentence above starts with a lot of complex terms and we don‚Äôt know their importance until the end of sentence, which is bad. The simple fix is just reversing the order!\n\n\nHowever, this paper is the first to analyze managed WebRTC services using SFU architecture and SDN-assisted IP multicasting of scalable video within WebRTC system\n\n\nWhat If I Want to Stress Multiple Ideas in a Sentence?\n\nTry to introduce just one major idea per sentence, especially if the idea is complex.\nIf your text is complex and you have two ideas worth emphasizing, create two sentences.\n\nExample:\nHowever, it uses the already limited upload bandwidth of clients inefficiently and is not scalable with the number of clients, i.e., it becomes impractical as the number of endpoints grows bigger.\nInstead of the sentence above, you can write:\nHowever, mesh topology uses the already limited upload bandwidth of clients inefficiently. It is not scalable with the number of clients, i.e., it becomes impractical as the number of endpoints grows bigger.\nSometimes two sentences should be one if they refer to the same idea.\nExample:\nSuch leaders should make the work of their followers more pleasant. Moreover, they should treat the followers as equals, and respect them.\nInstead of the sentence above, you can write:\nSuch leaders should make the work of their followers more pleasant by treating them equally and respectfully.\n\n\n\n\n\n\nTipAdditional Steps to Edit Complex Writing\n\n\n\n\nMove the subject and the verb close together.\nBreak apart sentences that contain too much new information.\nUse transitional phrases to indicate relationships: moreover, in addition, consequently, therefore‚Ä¶"
  },
  {
    "objectID": "posts/paragraph_dev_sentece_skill/index.html#run-ons",
    "href": "posts/paragraph_dev_sentece_skill/index.html#run-ons",
    "title": "Academic Writing Notes: Paragraphs Development and Sentence Skills",
    "section": "Run-Ons",
    "text": "Run-Ons\nA run-on is two complete thoughts run together with no sign to mark the break between them or with just a comma:\nThen, in [2], they also presented a bisection algorithm to compute \\(\\epsilon\\)-pseudospectral abscissa of a fixed matrix, i.e.¬†\\(\\alpha_{\\epsilon}(A)\\), and tried to compute minimum \\(\\epsilon\\)-pseudospectral abscissa over feasible matrices, however, an algorithm wasn‚Äôt presented yet.\nvs\nThen, in [2], they also presented a bisection algorithm to compute \\(\\epsilon\\)-pseudospectral abscissa of a fixed matrix, i.e.¬†\\(\\alpha_{\\epsilon}(A)\\). They also tried to compute minimum \\(\\epsilon\\)-pseudospectral abscissa over feasible matrices. However, an algorithm wasn‚Äôt presented yet.\n\n\n\n\n\n\nNote\n\n\n\nI personally didn‚Äôt like the second version too because it has too many ‚Äúthey also‚Äù parts."
  },
  {
    "objectID": "posts/paragraph_dev_sentece_skill/index.html#fragments",
    "href": "posts/paragraph_dev_sentece_skill/index.html#fragments",
    "title": "Academic Writing Notes: Paragraphs Development and Sentence Skills",
    "section": "Fragments",
    "text": "Fragments\nA sentence fragment is a group of words that lacks a subject or a verb and does not express a complete thought:\nPurdue offers many majors in engineering. Such as electrical, chemical, and industrial engineering.\nPurdue offers many majors in engineering such as electrical, chemical, and industrial engineering."
  },
  {
    "objectID": "posts/paragraph_dev_sentece_skill/index.html#parallelism",
    "href": "posts/paragraph_dev_sentece_skill/index.html#parallelism",
    "title": "Academic Writing Notes: Paragraphs Development and Sentence Skills",
    "section": "Parallelism",
    "text": "Parallelism\nWords in a pair or series should have a parallel structure.\nNot Parallel: The production manager was asked to write his report quickly, accurately, and in a detailed manner.\nParallel: The production manager was asked to write his report quickly, accurately, and thoroughly."
  },
  {
    "objectID": "posts/paragraph_dev_sentece_skill/index.html#misplaced-modifiers",
    "href": "posts/paragraph_dev_sentece_skill/index.html#misplaced-modifiers",
    "title": "Academic Writing Notes: Paragraphs Development and Sentence Skills",
    "section": "Misplaced Modifiers",
    "text": "Misplaced Modifiers\nMisplaced modifiers do not describe the word in the way the writer intended because of their wrong place in a sentence.\nGeorge couldn‚Äôt drive to work in his small sports car with a broken leg.\nWith a broken leg, George couldn‚Äôt drive to work in his small sports car.\nIn this example, we and transformer models know that George has a broken leg, not the car. But grammatically, the modifier ‚Äúwith a broken leg‚Äù seems to describe the car. This is an easy example but in a academic text, it can be more complex and harder to spot.\nIn order to avoid misplaced modifiers, place the words as close as possible to what they describe."
  },
  {
    "objectID": "posts/paragraph_dev_sentece_skill/index.html#dangling-modifiers",
    "href": "posts/paragraph_dev_sentece_skill/index.html#dangling-modifiers",
    "title": "Academic Writing Notes: Paragraphs Development and Sentence Skills",
    "section": "Dangling Modifiers",
    "text": "Dangling Modifiers\nA modifier that opens a sentence must be followed immediately by the word it is meant to describe. Otherwise the sentence takes on an unintended meaning.\nWhile smoking a pipe, my dog sat with me.\nWhile smoking a pipe, I sat with my dog.\nWhile I was smoking a pipe, my dog sat with me.\nAgain, this is also an easy example but in a academic text, it can be more complex and harder to spot."
  },
  {
    "objectID": "posts/paragraph_dev_sentece_skill/index.html#sentence-variety",
    "href": "posts/paragraph_dev_sentece_skill/index.html#sentence-variety",
    "title": "Academic Writing Notes: Paragraphs Development and Sentence Skills",
    "section": "Sentence Variety",
    "text": "Sentence Variety\n\nToo many sentences with the same structure and length can grow monotonous for readers.\nVarying sentence style and structure can also reduce repetition and add emphasis.\nLong sentences work well for incorporating a lot of information, and short sentences can often maximize crucial points.\n\n\nOverusing Long Sentences\nLong sentences can be difficult to read and understand, especially if they contain multiple ideas or clauses. Breaking up long sentences into shorter ones can improve clarity and readability.\nThe company reported that yearly profit growth, which had steadily increased by more than 7% since 1989, had stabilized in 2009 with a 0% comp, and in 2010, the year they launched the OWN project, actually decreased from the previous year by 2%. This announcement stunned Wall Street analysts, but with the overall decrease in similar company profit growth worldwide, as reported by Author (Year) in his article detailing the company‚Äôs history, the company‚Äôs announcement aligns with industry trends and future industry predictions.\nThe company reported that profit growth stabilized in 2009, though it had steadily increased by more than 7% since 1989. In 2010, the year they launch the OWN project, company profit growth decreased from the previous year. This announcement stunned Wall Street analysts. According to Author (Year), however, this decrease is exemplar of a trend across similar company profit growth worldwide; it also supports future predictions for the industry.\n\n\n\n\n\n\nNote\n\n\n\nNotice that the sentence count only increased by two, but thanks to the choice of where a sentence begin and end, the paragraph is easier to read. Moreover, the sentence variety is increased.\n\n\n\n\nShort Sentences\nRead the text below with your voice:\nToo many short sentences can hurt an essay. They can make the writing seem choppy. The writing may seem like it is below a college level. Readers may lose interest. They may not want to continue reading.\nSee the effect? Let‚Äôs fix it:\nToo many short sentences can hurt an essay, for it can make the writing seem choppy and below a college level. Because of this, readers may lose interest and not want to continue reading."
  },
  {
    "objectID": "posts/paragraph_dev_sentece_skill/index.html#change-the-rhythm",
    "href": "posts/paragraph_dev_sentece_skill/index.html#change-the-rhythm",
    "title": "Academic Writing Notes: Paragraphs Development and Sentence Skills",
    "section": "Change the Rhythm!",
    "text": "Change the Rhythm!\nChange the rhythm of your writing by varying sentence length and structure. As you will see the example below, varying sentence length and structure can make your writing more interesting and engaging.\nVary the rhythm by alternating short and long sentences:\nThe Winslow family visited Canada and Alaska last summer to find some Native American art. In Anchorage stores they found some excellent examples of soapstone carvings. But they couldn‚Äôt find a dealer selling any of the woven wall hangings they wanted. They were very disappointed when they left Anchorage empty-handed.\nThe Winslow family visited Canada and Alaska last summer to find some native American art, such as soapstone carvings and wall hangings. Anchorage stores had many soapstone items available. Still, they were disappointed to learn that wall hangings, which they had especially wanted, were difficult to find. Sadly, they left empty-handed.\nI think I see something similar to this in the novels I‚Äôve read."
  },
  {
    "objectID": "posts/paragraph_dev_sentece_skill/index.html#repeated-subjects-or-topics",
    "href": "posts/paragraph_dev_sentece_skill/index.html#repeated-subjects-or-topics",
    "title": "Academic Writing Notes: Paragraphs Development and Sentence Skills",
    "section": "Repeated Subjects or Topics",
    "text": "Repeated Subjects or Topics\n\nHandling the same topic for several sentences can lead to repetitive sentences. When that happens, consider using these parts of speech to fix the problem:\n\nRelative pronouns\nIndiana used to be mainly an agricultural state. It has recently attracted more industry. Indiana, which used to be mainly an agricultural state, has recently attracted more industry.\nParticiples\nWei Xie was surprised to get a phone call from his sister. He was happy to hear her voice again. Surprised to get a phone call from his sister, Wei Xie was happy to hear her voice again.\nPrepositions\nThe university has been facing pressure to cut its budget. It has eliminated funding for important programs. Under pressure to cut its budget, the university has eliminated funding for important programs."
  },
  {
    "objectID": "posts/paragraph_dev_sentece_skill/index.html#final-words",
    "href": "posts/paragraph_dev_sentece_skill/index.html#final-words",
    "title": "Academic Writing Notes: Paragraphs Development and Sentence Skills",
    "section": "Final Words",
    "text": "Final Words\nFinally, all these rules and tips are not strict rules, but guidelines to help you improve your writing. Also, it is very easy to forget them. The best thing you can do is keep writing constantly, while editing your own writing with these rules in mind. Over time, you will internalize these rules and your writing will improve. I also believe that reading a lot of well-written articles and books will help you improve your writing skills."
  },
  {
    "objectID": "posts/paragraph_dev_sentece_skill/index.html#references",
    "href": "posts/paragraph_dev_sentece_skill/index.html#references",
    "title": "Academic Writing Notes: Paragraphs Development and Sentence Skills",
    "section": "References",
    "text": "References\n\nKoc University Writing Center\nSentence Variety. (2018). Retrieved from https://owl.purdue.edu/owl/general_writing/academic_writing/sentence_variety/index.html\nMaking Complex Writing Intelligible with Known-New Contract. (2018). Global Communication Center, Carnegie Mellon University. Retrieved from https://www.cmu.edu/gcc/handouts/old-new-handout-pdf"
  }
]